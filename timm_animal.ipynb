{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674c435a-cac8-4e61-8a96-aa9e91de4d9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Scene Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5420c015-b9c1-4d94-8227-6265aff162c0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 500, 500])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timm.data.transforms_factory import create_transform\n",
    "from timm.data import create_dataset\n",
    "\n",
    "# create_dataset reads \"train\" folder and \"val\" folder.\n",
    "# the folder name must be matched.\n",
    "myds = create_dataset(name='', root = '/media/nd-ygr/Data/Image/SceneImage_Intel/sceneimages.tar', transform=create_transform(500), split='train')\n",
    "\n",
    "image, label = myds[0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11f5b06c-7885-483d-b565-81604a6df197",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "transform = T.ToPILImage()\n",
    "imgview = transform(image)\n",
    "imgview.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a3642c5-a230-413b-bbd9-0f80616a7fe9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<timm.data.dataset.ImageDataset at 0x7fb39b5e9d30>,\n",
       " timm.data.dataset.ImageDataset)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myds, type(myds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54eba72d-f345-40e9-a9e7-c57c715729d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14034\n",
      "3000\n",
      "7301\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "# Dataset Path List\n",
    "path = \"/media/nd-ygr/Data/Image/SceneImage_Intel\"\n",
    "\n",
    "trainset_path = glob(f'{path}/train/*/*.jpg')\n",
    "valset_path = glob(f'{path}/val/*/*.jpg')\n",
    "predset_path = glob(f'{path}/pred/*.jpg')\n",
    "\n",
    "print(len(trainset_path))    #14034\n",
    "print(len(valset_path))      # 3000\n",
    "print(len(predset_path))     # 7301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9a60cc-31b1-4487-91fe-75a986d88b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset_path[0]\n",
    "# '/media/nd-ygr/Data/Image/SceneImage_Intel/train/buildings/1291.jpg'\n",
    "\n",
    "def get_label(label_path_list):\n",
    "    labels = []\n",
    "    for label_path in label_path_list:\n",
    "        label = label_path.split(\"/\")[-2]\n",
    "        labels.append(label)\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b03bed0-9b45-47e3-98f1-b6bd1a015ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c57f6330-2b67-4e00-9df5-959b0ac45632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    image_list : 이미지 경로 리스트\n",
    "    label_list : 이미지 레이블 리스트\n",
    "    transform = 전처리\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_list, classes, transform):\n",
    "        self.image_list = image_list\n",
    "        self.label_list = get_label(image_list)\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label = self.label_list[index]\n",
    "        image_path = self.image_list[index]\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform is not None:\n",
    "            transformed_image = self.transform(image)\n",
    "        \n",
    "        return transformed_image, self.classes.index(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c1dc75-21a4-49da-b3c2-87bac5e02d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class MyTransform():\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.image_transform = transforms.Compose(\n",
    "            [transforms.Resize((size,size), transforms.InterpolationMode.BICUBIC),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        \n",
    "        return self.image_transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c054335-1037-49ca-be44-6b9f0aa5ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Batch Size\n",
    "batch_size = 12\n",
    "number_of_labels = 6\n",
    "\n",
    "# Define Classes\n",
    "classes = ('buildings', 'forest', 'glacier', 'mountain', 'sea', 'street')\n",
    "\n",
    "img_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56115ab8-de57-4a4a-8b06-c4c93dfbd635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images in a training set :  14040\n",
      "The number of images in a validation set : 3000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainloader = DataLoader(MyDataset(trainset_path, classes, MyTransform(img_size)), batch_size=batch_size, shuffle=True)\n",
    "print(\"The number of images in a training set : \", len(trainloader)*batch_size)\n",
    "\n",
    "valloader = DataLoader(MyDataset(valset_path, classes, MyTransform(img_size)), batch_size=batch_size, shuffle = True)\n",
    "print(f\"The number of images in a validation set : {len(valloader)*batch_size}\")\n",
    "\n",
    "# testloader = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb7dd08-2208-417c-9881-f33b370c8d4b",
   "metadata": {},
   "source": [
    "## Load classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ccfcbf9-08e0-4309-9c17-47afba1ad3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77d829c2-a6f2-4868-9b25-d97fc2751147",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('resnet50d', pretrained = True, num_classes = len(classes), global_pool = 'catavgmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a6f5f7e-65f1-4363-8f82-e27acba626c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.get_classifier() :  Linear(in_features=4096, out_features=6, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fc layer 없는 상태\n",
    "print(\"model.get_classifier() : \", model.get_classifier())\n",
    "num_in_features = model.get_classifier().in_features; num_in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14dd8ef3-7c19-4f23-bbbf-1a455259ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "dropout = 0.4\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.BatchNorm1d(num_in_features),\n",
    "    nn.Linear(in_features = num_in_features, out_features = 512, bias = False),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(in_features = 512, out_features = 6, bias = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa07c7c2-46a3-4cd0-9fb7-feb239926d6a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d (pool_type=catavgmax, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (fc): Sequential(\n",
       "    (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Linear(in_features=4096, out_features=512, bias=False)\n",
       "    (2): ReLU()\n",
       "    (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Dropout(p=0.4, inplace=False)\n",
       "    (5): Linear(in_features=512, out_features=6, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델의 마지막 레이어에 커스텀 레이어가 적용된 모습을 볼 수 있다.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a91fac39-7ab3-4c64-98c6-afa7cccee119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model):\n",
    "    \n",
    "    total = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in valloader:\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "            \n",
    "        accuracy = (100 * accuracy) / total\n",
    "        \n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "851219a8-91fe-47ac-b1ba-83aaf6fcf453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs):\n",
    "    \n",
    "    # Convert model parameters and buffers to CPU or Cuda\n",
    "    model = model.to(device)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    from torch.optim import Adam\n",
    "    optimizer = Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (images, labels) in enumerate(trainloader, 0):\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # print(\"labels : \", labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            # print(type(outputs))\n",
    "            # print(outputs)\n",
    "            # print(outputs.data)\n",
    "            # _, prediction = torch.max(outputs, 1)\n",
    "            # print(\"prediction : \", prediction)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()        \n",
    "        \n",
    "        if divmod(epoch+1, 2)[1] == 0:\n",
    "            val_accuracy = validation(model)\n",
    "            print(\"[%3d / %3d] train loss : %.3f, validation accuracy : %.3f\" % (epoch+1, num_epochs, running_loss/100, val_accuracy))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            if val_accuracy > best_accuracy:\n",
    "                saveModel(model)\n",
    "                best_accuracy = val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b7f0c54-2544-4b82-9186-be0b2da1062a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valAccuracy(model):\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for val in valloader:\n",
    "            \n",
    "            images, labels = val\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "            \n",
    "        accuracy = (100 * accuracy / total)\n",
    "        \n",
    "    return accuracy\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9f0a668-6a35-4252-b7e3-2c95f2d06e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model):\n",
    "    path = './timm_result/timm_sceneClassModel.pth'\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86747617-7158-4898-93ef-1efc3dbc741c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be running on cuda device\n",
      "[  2 /  20] train loss : 2.033, validation accuracy : 93.033\n",
      "[  4 /  20] train loss : 1.214, validation accuracy : 93.000\n",
      "[  6 /  20] train loss : 0.652, validation accuracy : 92.100\n",
      "[  8 /  20] train loss : 0.499, validation accuracy : 92.767\n",
      "[ 10 /  20] train loss : 0.424, validation accuracy : 90.500\n",
      "[ 12 /  20] train loss : 0.410, validation accuracy : 92.833\n",
      "[ 14 /  20] train loss : 0.294, validation accuracy : 92.933\n",
      "[ 16 /  20] train loss : 0.389, validation accuracy : 92.800\n",
      "[ 18 /  20] train loss : 0.366, validation accuracy : 90.533\n",
      "[ 20 /  20] train loss : 0.341, validation accuracy : 92.900\n"
     ]
    }
   ],
   "source": [
    "# Define your execution device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"The model will be running on\", device, \"device\")\n",
    "\n",
    "train(model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8dd59-810c-42d0-b6c5-4163672a3a2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check feature maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94bcd48b-e2d3-4fda-8aa5-0e9eb8f99f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 500, 500])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "image = torch.as_tensor(np.array(image, dtype=np.float32)).transpose(0,0)[None]\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13fef5e3-a4fe-4fcd-9a01-ad844831b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_output= model.forward_features(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be787cf0-a7ca-4fb8-b82f-953216cb4d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_feature_output(t):\n",
    "    plt.imshow(t[0].transpose(0,2).sum(-1).detach().numpy())\n",
    "    plt.show()\n",
    "    \n",
    "visualize_feature_output(feature_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b32dd-ffb3-46df-ab4f-3d1ec8bf9727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
